{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Knowledge Base","text":"<p>A place that captures all the things that I would like to put down. A knowledge base for my own sake.</p> <p>A good way to test if you've learnt a knowledge is when you can explain the same concept in your own words.</p>"},{"location":"database/database-internal/","title":"Database Internal","text":""},{"location":"database/database-internal/#components","title":"Components","text":"<p>A database is usually consist of two parts - a frontend that talks to the users, and a storage engine that persists the data.</p>"},{"location":"database/database-internal/#frontend","title":"Frontend","text":"<p>A database frontend defines a couple things - the APIs that a user can use to query the data or execute the commands, and the data format that presents the data to the users.</p>"},{"location":"database/database-internal/#storage-engine","title":"Storage Engine","text":"<p>A storage engine is responsible for persisting the data to the disk and extracting the data from the disk.</p> <p>A storage engine is also usually tasked to implement indexes, ensure the ACID in transactions, and persist the WAL records.</p>"},{"location":"database/database-internal/#sql-vs-nosql","title":"SQL vs. NoSQL","text":"<p>To compare SQL and NoSQL from the inside, they are mainly different in how data is stored and presents to the users.</p> <p>In SQL, data is stored in tables with columns and rows. A table with a large amount of records/rows, has its records written into Pages first before they are flushed into the disk. Similarly, when loading data from the disk, the storage engine loads a subset of data into a Page (i.e. the first Page), filter out the results, and then continue with the next Page. Thus, data will be loaded frequently from the disk to a Page for a large data set.</p> <p>In NoSQL, data is stored in documents - i.e. not restricted to structured data. One document represents a record of data that can be retrieved as a whole. The data retrieval of a NoSQL database can usually load the data needed as many as possible into a Page, and thus improves the efficiency by reducing the need of going back and forth between the disk and Pages. And thus, NoSQL database usually excels in high thoughput requirements.</p>"},{"location":"database/mongodb/","title":"MongoDB","text":"<p>Knowledge collected from the article and presentation by Hussein Nasser.</p>"},{"location":"database/mongodb/#how-it-works-internally","title":"How It Works Internally","text":"<p>MongoDB is a document based database, where schema is not strictly defined between documents in a collection - i.e. one field(s) can exist in one document but not the others in the same collection.</p>"},{"location":"database/mongodb/#binary-json-bson","title":"Binary JSON (BSON)","text":"<p>BSON is the format that stores the JSON document in the disk. [Reference from the official doc]</p> <p>BSON stands for \u201cBinary JSON,\u201d and that\u2019s exactly what it was invented to be. BSON\u2019s binary structure encodes type and length information, which allows it to be traversed much more quickly compared to JSON.</p>"},{"location":"database/mongodb/#the-_id-field","title":"The <code>_id</code> field","text":"<p>An <code>_id</code> is created when a document is created in a collection, which is a field with a size of 12 bytes. The reason why it's large is because MongoDB tries to make it globaly unique across machines or shards for scaling purposes. This field is used to locate a document eventually, but the implementation of how the <code>_id</code> field map to a document evolves in different versions.</p>"},{"location":"database/mongodb/#secondary-index","title":"Secondary Index","text":"<p>The secondary index is used to accelerate the lookup of specific information in a document. Similarly, a secondary index locates a document eventually based on the specified field, but the implementation varies during evolution of the Storage Engine in MongoDB.</p>"},{"location":"database/mongodb/#mmap_v1-42","title":"MMAP_v1 (&lt; 4.2)","text":"<p>MMAP_v1 is the first version of the MongoDB Storage Engine, which maps the <code>_id</code> field to a <code>Diskloc</code> that locates a document in the disk with a filename and an offset. The downside of this design is that the offset needs to be maintained carefully for each update, otherwise the lookup of documents will be off.</p> <p>Another downside is that it only allows a single write to the DB at a time, where a global lock is required when it happens, which could lead to performance concerns as it scales.</p>"},{"location":"database/mongodb/#wiredtiger-v1-42-52","title":"WiredTiger v1 (4.2 - 5.2)","text":"<p>WiredTiger is a Storage Engine acquired by MongoDB. It comes with better performance where two concurrent writes to different documents in the same collection becomes possible.</p> <p>This implementation inroduced a hidden clustered index where the actions required to locate a document involved two B+ Tree lookups. The <code>_id</code> field is firstly used to locate a <code>recordId</code>, which then is used in the hidden clustered index to locate the corresponding document. With this design, the storage engine is able to load more documents with fewer I/Os since the <code>recordId</code> is relatively small - i.e. 64 bit.</p> <p>A clustered index is an index where a lookup gives you all what you need, all fields are stored in the leaf page resulting in what is commonly known in database systems as Index-only scans.</p> <p>However, the downside is that each operation now requires two B+ Tree lookups, which may require more computer resources to operate on, which could lead to performance concerns.</p>"},{"location":"database/mongodb/#wiredtiger-v2-53","title":"WiredTiger v2 (&gt; 5.3)","text":"<p>An upgraded version of this storage engine removes the lookup between the <code>_id</code> index and the <code>recordId</code>, and instead, maintains the <code>_id</code> index and the documents in a clustered index such that a lookup with primary index only needs to traverse one B+ Tree. This architecture is called Clustered Collections Architecture.</p> <p>The secondary index though now has to point to the <code>_id</code> field with this option. Since the <code>_id</code> field is a field with a size of 12 bytes, this design results in loading a lot more data in secondary indexes, comparing to the v1 design where the <code>recordId</code> is only 64 bits. This also means that the secondary index would still have to do two lookups in this design.</p> <p>However, MongoDB gives users the option to cluster the collections or not, which would be a design tradeoff to think about when using MongoDB.</p>"},{"location":"database/performance-tips/","title":"Performance Tips","text":""},{"location":"database/performance-tips/#performance-tips","title":"Performance Tips","text":"<p>The following tips for performance improvement is not tight to a specific DB solution. They can be applied to many of the DB solutions nowadays.</p>"},{"location":"database/performance-tips/#prepared-statements","title":"Prepared Statements","text":"<p>Like GraphQL, each document attached in a request needs to be parsed and validated before execution. If a statement is prepared, the same requests that come with the same statement can skip this process, and thus improves the performance.</p> <p>However, prepared statements only work in the same session. If a statement is not prepared in a session, it needs to be prepared before execution.</p> <p>This also doesn't help in performance if a statement itself is time consuming - e.g. loading a large amount of data.</p>"},{"location":"database/performance-tips/#index","title":"Index","text":"<p>This is also a common technique in database to accelerate data lookup. This technique works similarly to a hash table where the key is generated by the specified column, and the value is the corresponding row of records. A common implementation of Index is B+ Tree. However, how an index maps to a record could be various in different DB implementation.</p>"},{"location":"database/performance-tips/#partition","title":"Partition","text":"<p>This technique segments the data into different partitions according to the column(s) specified, such that queries can scan only the partitions of data that are interested without scanning through all the records one by one.</p> <p>However, be sure to benchmark the performance before determining which columns to use for partitioning. Scanning through a large amount of partitions would not result in performance improvement.</p> <p>Also, be sure to keep the partitions up-to-date when data are updated in the database. A common practice is to leverage cron job to regularly update the partitions created such that queries run against the partitions would not be performed with stale data.</p>"},{"location":"database/performance-tips/#copy","title":"Copy","text":"<p>To improve INSERT performance for a large amount of data, the COPY command can be used to import the data from either a file (e.g. a csv file) or standard input to the database.</p>"},{"location":"database/performance-tips/#separation-of-concerns","title":"Separation of Concerns","text":"<p>This is basically an idea of horizontal scaling, where READ replicas are created to take care of the READ concerns specifically, such that the main instance of the DB can focus on scaling for the WRITE operations. This is usually a technique used when the amount of READ operations is significantly larger than the WRITE operations.</p>"},{"location":"database/postgresql/","title":"PostgreSQL","text":""},{"location":"database/postgresql/#specialties","title":"Specialties","text":"<ul> <li>Multi-version concurrency control<ul> <li>i.e. A row of records is versioned into multiple tuples that are representing the same logical row but in different states/versions. The newest version represents the up-to-date record for that row. Different processes can access a record concurrently.</li> </ul> </li> <li>Can define custom type<ul> <li>ProgreSQL allows users to define custome types/objects in a table schema.</li> </ul> </li> </ul>"},{"location":"database/postgresql/#how-it-works-internally","title":"How It Works Internally","text":"<p>Knowledge collected from presentation by Hussein Nasser.</p>"},{"location":"database/postgresql/#overview","title":"Overview","text":""},{"location":"database/postgresql/#post-master-process","title":"Post Master Process","text":"<p>A parent process that starts at the early stage of the application. Exposes the application to port 5432, such that it's ready for connection.</p>"},{"location":"database/postgresql/#backend-processes","title":"Backend Processes","text":"<p>Each backend process is responsible for maintaining a connection to its consumer.</p> <p>The number of backend process is capped by the parameter <code>max_connections</code>.</p>"},{"location":"database/postgresql/#background-workers","title":"Background Workers","text":"<p>A background worker is responsible for executing the query or command that a consumer initiated.</p> <p>The number of background workers is capped by the parameter <code>max_worker_processes</code>.</p>"},{"location":"database/postgresql/#background-writers","title":"Background Writers","text":"<p>A background writer is responsible for flushing the data that is stored in a Page into the filesystem, which will eventually write the data into the disk. It wakes up occasionally to clean up dirty Pages/Shared Memory.</p>"},{"location":"database/postgresql/#checkpointer","title":"Checkpointer","text":"<p>Checkpointer is responsible for flushing everything - i.e. both WAL records and Pages to the disk, and creating a checkpoint, indicating that everything now is consistent.</p>"},{"location":"database/postgresql/#logger","title":"Logger","text":"<p>Logger is responsible for writing all the logs into the disk.</p>"},{"location":"database/postgresql/#autovacuum-launcher-and-workers","title":"Autovacuum Launcher and Workers","text":"<p>Autovacuum worker is responsible for cleaning up the tuples that have older versions and are not being accessed by any of the processes.</p> <p>The number of autovacuum works is capped by the parameter <code>autovacuum_max_workers</code>.</p>"},{"location":"database/postgresql/#wal-archiver","title":"WAL Archiver","text":"<p>WAL Archiver is responsible for archiving all the WAL records for historical purposes, such that a database can be brought to the newest state from 0 when needed.</p>"},{"location":"database/postgresql/#wal-reciever","title":"WAL Reciever","text":"<p>WAL Reciever is responsible for recieving WAL records.</p>"},{"location":"database/postgresql/#wal-writer","title":"WAL Writer","text":"<p>WAL Writer is responsible for flushing the WAL records into the disk. Each time a commit succeeds, WAL records are flushed into the disk.</p>"},{"location":"database/postgresql/#wal-sender","title":"WAL Sender","text":"<p>A WAL Sender is responsible for sending the WAL records to the replicas, from the master instance.</p> <p>The number of WAL Senders is capped by the parameter <code>max_val_senders</code>.</p>"},{"location":"database/postgresql/#startup-process","title":"Startup Process","text":"<p>The Startup Process is responsible for checking the state of the database - i.e. it checks whether the data stored in the Pages are up-to-date with what has been recorded in the WAL records. If the Pages are found out of date, the Startup process will try to re-do what has been recorded in the WAL records, in order to load the up-to-date data into the Pages, before the Post Master Process is started.</p>"},{"location":"dsa/b-plus-tree/","title":"B+ Tree","text":"<p>B+ Tree is an advanced usage of B Tree.</p>"},{"location":"dsa/b-plus-tree/#properties","title":"Properties","text":"<p>Other than the properties maintained in a B Tree, a B+ Tree also comes with the following properties:</p> <ul> <li>The data pointers are all stored in the leaf nodes, while a B Tree may store both the data and the keys in each internal node.</li> <li>The internal nodes in a B+ Tree are used to guide the search.</li> <li>All leaf nodes form a linked list for efficient range-based queries.</li> <li>A B+ Tree usually comes with higher order - i.e. more keys in each node.</li> <li>A B+ Tree allows key duplication in leaf nodes, while a B Tree does not allow.</li> <li>A B+ Tree requires less disk I/O, since it supports sequential reads with the linked list structure in the leaf nodes.</li> </ul>"},{"location":"dsa/b-tree/","title":"B Tree","text":"<p>A balanced binary search tree that improves search and insertion operations with less memory and I/Os.</p> <p>Each node in the tree can contains multiple keys that allows the tree to have large branching factor. And thus, the tree can come with smaller height.</p> <p>A B Tree algorithm copies selected blocks from the disk to the memory, and writes back the blocks that have changed onto disk. It only keeps a constant number of blocks in the memory at any time, and thus does not limit the size of the tree that can be handled.</p>"},{"location":"dsa/b-tree/#properties","title":"Properties","text":"<p>Assuming that there is a B Tree with an order of <code>m</code>, and a minimum degree of <code>t</code> (which is usually defined by the block size of a disk):</p> <ul> <li>Each internal node (excluding root) has the number of child nodes within the range of <code>[ceil(m / 2), m]</code>, or <code>[t, 2t]</code>.</li> <li>The root node can have either 2 nodes, or 0 - if the tree has only one node, which is the root.</li> <li>The number of keys contained in each node equals to <code>the number of child nodes - 1</code>.</li> <li>The root node may contain a minimum of 1 key.</li> <li>All keys of a node are sorted in increasing order.</li> <li>The range of keys follows the property of a Binary Search Tree where the range of keys within a child node is bounded by the range of keys within its parent node. For example:<ul> <li>childNode[0].keys[i] &lt; parentNode.keys[0], i.e. the first branch of a node may only contains keys that are less than the first key in the node.</li> <li>childNode[1].keys[i] &gt; parentNode.keys[0] &amp;&amp; childNode[1].keys[i] &lt; parentNode.keys[1], i.e. the second branch of a node may only contains keys that are greater than the first key in the node and less than the second key in the node.</li> <li>and so on.</li> </ul> </li> <li>All leaf nodes are at the same level.</li> <li>The time complexity for search, insertion and deletion operation is O(logn).</li> </ul>"},{"location":"miscellaneous/ditching-typescript/","title":"Big Projects Ditching TypeScript, and Why?","text":"<p>This is an interesting point of view, and I would like to put it down somewhere for future references, if need to.</p> <p>It's a video from Youtube that caught my eyes: Big projects are ditching TypeScript\u2026 why?.</p> <p>I personally have got the chance to learn TypeScript and use it in projects. But the first time I heard about the idea, I personally liked it.</p> <p>I come from a strongly typed programming language background, and would think that any tool that can help me identify a potential type error during dev or compile time would be helpful, such that such errors can be caught before runtime.</p> <p>While TypeScript does the job, the main points that were mentioned in the video were that:</p> <ul> <li>It pollutes the code with type gymnastics.</li> <li>It requires explicit compile step.</li> </ul> <p>I kinda agreed with the first point when I firstly heard about how to define types with TypeScript. But I was also viewing that as a tradeoff to introduce type safety in Javascript.</p> <p>I don't have a lot experience and data to tell how much of a problem the second point can bring in. But it's something that can be benchmarked.</p> <p>An alternative that was mentioned in the video is <code>JSDoc</code>.</p> <p>I think what is worth noting is that ECMAScript is having a proposal that aims to enable developers to add type annotations to the Javascript code. When it becomes a standard, it'll be interesting to compare the option with TypeScript and JSDoc, and see what areas each of those excels at.</p>"}]}