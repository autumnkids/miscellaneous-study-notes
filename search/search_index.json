{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Knowledge Base","text":"<p>A place that captures all the things that I would like to put down. A knowledge base for my own sake.</p> <p>A good way to test if you've learnt a knowledge is when you can explain the same concept in your own words.</p>"},{"location":"database/database-internal/","title":"Database Internal","text":""},{"location":"database/database-internal/#components","title":"Components","text":"<p>A database is usually consist of two parts - a frontend that talks to the users, and a storage engine that persists the data.</p>"},{"location":"database/database-internal/#frontend","title":"Frontend","text":"<p>A database frontend defines a couple things - the APIs that a user can use to query the data or execute the commands, and the data format that presents the data to the users.</p>"},{"location":"database/database-internal/#storage-engine","title":"Storage Engine","text":"<p>A storage engine is responsible for persisting the data to the disk and extracting the data from the disk.</p> <p>A storage engine is also usually tasked to implement indexes, ensure the ACID in transactions, and persist the WAL records.</p>"},{"location":"database/database-internal/#sql-vs-nosql","title":"SQL vs. NoSQL","text":"<p>To compare SQL and NoSQL from the inside, they are mainly different in how data is stored and presents to the users.</p> <p>In SQL, data is stored in tables with columns and rows. A table with a large amount of records/rows, has its records written into Pages first before they are flushed into the disk. Similarly, when loading data from the disk, the storage engine loads a subset of data into a Page (i.e. the first Page), filter out the results, and then continue with the next Page. Thus, data will be loaded frequently from the disk to a Page for a large data set.</p> <p>In NoSQL, data is stored in documents - i.e. not restricted to structured data. One document represents a record of data that can be retrieved as a whole. The data retrieval of a NoSQL database can usually load the data needed as many as possible into a Page, and thus improves the efficiency by reducing the need of going back and forth between the disk and Pages. And thus, NoSQL database usually excels in high thoughput requirements.</p>"},{"location":"database/mongodb/","title":"MongoDB","text":"<p>Knowledge collected from the article and presentation by Hussein Nasser.</p>"},{"location":"database/mongodb/#how-it-works-internally","title":"How It Works Internally","text":""},{"location":"database/performance-tips/","title":"Performance Tips","text":""},{"location":"database/performance-tips/#performance-tips","title":"Performance Tips","text":"<p>The following tips for performance improvement is not tight to a specific DB solution. They can be applied to many of the DB solutions nowadays.</p>"},{"location":"database/performance-tips/#prepared-statements","title":"Prepared Statements","text":"<p>Like GraphQL, each document attached in a request needs to be parsed and validated before execution. If a statement is prepared, the same requests that come with the same statement can skip this process, and thus improves the performance.</p> <p>However, prepared statements only work in the same session. If a statement is not prepared in a session, it needs to be prepared before execution.</p> <p>This also doesn't help in performance if a statement itself is time consuming - e.g. loading a large amount of data.</p>"},{"location":"database/performance-tips/#index","title":"Index","text":"<p>This is also a common technique in database to accelerate data lookup. This technique works similarly to a hash table where the key is generated by the specified column, and the value is the corresponding row of records. A common implementation of Index is B+ Tree.</p>"},{"location":"database/performance-tips/#partition","title":"Partition","text":"<p>This technique segments the data into different partitions according to the column(s) specified, such that queries can scan only the partitions of data that are interested without scanning through all the records one by one.</p> <p>However, be sure to benchmark the performance before determining which columns to use for partitioning. Scanning through a large amount of partitions would not result in performance improvement.</p> <p>Also, be sure to keep the partitions up-to-date when data are updated in the database. A common practice is to leverage cron job to regularly update the partitions created such that queries run against the partitions would not be performed with stale data.</p>"},{"location":"database/performance-tips/#copy","title":"Copy","text":"<p>To improve INSERT performance for a large amount of data, the COPY command can be used to import the data from either a file (e.g. a csv file) or standard input to the database.</p>"},{"location":"database/performance-tips/#separation-of-concerns","title":"Separation of Concerns","text":"<p>This is basically an idea of horizontal scaling, where READ replicas are created to take care of the READ concerns specifically, such that the main instance of the DB can focus on scaling for the WRITE operations. This is usually a technique used when the amount of READ operations is significantly larger than the WRITE operations.</p>"},{"location":"database/postgresql/","title":"PostgreSQL","text":""},{"location":"database/postgresql/#specialties","title":"Specialties","text":"<ul> <li>Multi-version concurrency control<ul> <li>i.e. A row of records is versioned into multiple tuples that are representing the same logical row but in different states/versions. The newest version represents the up-to-date record for that row. Different processes can access a record concurrently.</li> </ul> </li> <li>Can define custom type<ul> <li>ProgreSQL allows users to define custome types/objects in a table schema.</li> </ul> </li> </ul>"},{"location":"database/postgresql/#how-it-works-internally","title":"How It Works Internally","text":"<p>Knowledge collected from presentation by Hussein Nasser.</p>"},{"location":"database/postgresql/#overview","title":"Overview","text":""},{"location":"database/postgresql/#post-master-process","title":"Post Master Process","text":"<p>A parent process that starts at the early stage of the application. Exposes the application to port 5432, such that it's ready for connection.</p>"},{"location":"database/postgresql/#backend-processes","title":"Backend Processes","text":"<p>Each backend process is responsible for maintaining a connection to its consumer.</p> <p>The number of backend process is capped by the parameter <code>max_connections</code>.</p>"},{"location":"database/postgresql/#background-workers","title":"Background Workers","text":"<p>A background worker is responsible for executing the query or command that a consumer initiated.</p> <p>The number of background workers is capped by the parameter <code>max_worker_processes</code>.</p>"},{"location":"database/postgresql/#background-writers","title":"Background Writers","text":"<p>A background writer is responsible for flushing the data that is stored in a Page into the filesystem, which will eventually write the data into the disk. It wakes up occasionally to clean up dirty Pages/Shared Memory.</p>"},{"location":"database/postgresql/#checkpointer","title":"Checkpointer","text":"<p>Checkpointer is responsible for flushing everything - i.e. both WAL records and Pages to the disk, and creating a checkpoint, indicating that everything now is consistent.</p>"},{"location":"database/postgresql/#logger","title":"Logger","text":"<p>Logger is responsible for writing all the logs into the disk.</p>"},{"location":"database/postgresql/#autovacuum-launcher-and-workers","title":"Autovacuum Launcher and Workers","text":"<p>Autovacuum worker is responsible for cleaning up the tuples that have older versions and are not being accessed by any of the processes.</p> <p>The number of autovacuum works is capped by the parameter <code>autovacuum_max_workers</code>.</p>"},{"location":"database/postgresql/#wal-archiver","title":"WAL Archiver","text":"<p>WAL Archiver is responsible for archiving all the WAL records for historical purposes, such that a database can be brought to the newest state from 0 when needed.</p>"},{"location":"database/postgresql/#wal-reciever","title":"WAL Reciever","text":"<p>WAL Reciever is responsible for recieving WAL records.</p>"},{"location":"database/postgresql/#wal-writer","title":"WAL Writer","text":"<p>WAL Writer is responsible for flushing the WAL records into the disk. Each time a commit succeeds, WAL records are flushed into the disk.</p>"},{"location":"database/postgresql/#wal-sender","title":"WAL Sender","text":"<p>A WAL Sender is responsible for sending the WAL records to the replicas, from the master instance.</p> <p>The number of WAL Senders is capped by the parameter <code>max_val_senders</code>.</p>"},{"location":"database/postgresql/#startup-process","title":"Startup Process","text":"<p>The Startup Process is responsible for checking the state of the database - i.e. it checks whether the data stored in the Pages are up-to-date with what has been recorded in the WAL records. If the Pages are found out of date, the Startup process will try to re-do what has been recorded in the WAL records, in order to load the up-to-date data into the Pages, before the Post Master Process is started.</p>"},{"location":"dsa/b-tree/","title":"B Tree","text":"<p>A balanced binary search tree that improves search and insertion operations with less memory and I/Os.</p> <p>Each node in the tree can contains multiple keys that allows the tree to have large branching factor. And thus, the tree can come with smaller height.</p> <p>A B Tree algorithm copies selected blocks from the disk to the memory, and writes back the blocks that have changed onto disk. It only keeps a constant number of blocks in the memory at any time, and thus does not limit the size of the tree that can be handled.</p>"},{"location":"dsa/b-tree/#properties","title":"Properties","text":"<p>Assuming that there is a B Tree with an order of <code>m</code>, and a minimum degree of <code>t</code> (which is usually defined by the block size of a disk):</p> <ul> <li>Each internal node (excluding root) has the number of child nodes within the range of <code>[ceil(m / 2), m]</code>, or <code>[t, 2t]</code>.</li> <li>The root node can have either 2 nodes, or 0 - if the tree has only one node, which is the root.</li> <li>The number of keys contained in each node equals to <code>the number of child nodes - 1</code>.</li> <li>The root node may contain a minimum of 1 key.</li> <li>All keys of a node are sorted in increasing order.</li> <li>The range of keys follows the property of a Binary Search Tree where the range of keys within a child node is bounded by the range of keys within its parent node. For example:<ul> <li>childNode[0].keys[i] &lt; parentNode.keys[0], i.e. the first branch of a node may only contains keys that are less than the first key in the node.</li> <li>childNode[1].keys[i] &gt; parentNode.keys[0] &amp;&amp; childNode[1].keys[i] &lt; parentNode.keys[1], i.e. the second branch of a node may only contains keys that are greater than the first key in the node and less than the second key in the node.</li> <li>and so on.</li> </ul> </li> <li>All leaf nodes are at the same level.</li> <li>The time complexity for search, insertion and deletion operation is O(logn).</li> </ul>"},{"location":"dsa/b-tree/#b-tree_1","title":"B+ Tree","text":"<p>B+ Tree is an advanced usage of B Tree.</p>"},{"location":"dsa/b-tree/#properties_1","title":"Properties","text":"<p>Other than the properties maintained in a B Tree, a B+ Tree also comes with the following properties:</p> <ul> <li>The data pointers are all stored in the leaf nodes, while a B Tree may store both the data and the keys in each internal node.</li> <li>The internal nodes in a B+ Tree are used to guide the search.</li> <li>All leaf nodes form a linked list for efficient range-based queries.</li> <li>A B+ Tree usually comes with higher order - i.e. more keys in each node.</li> <li>A B+ Tree allows key duplication in leaf nodes, while a B Tree does not allow.</li> <li>A B+ Tree requires less disk I/O, since it supports sequential reads with the linked list structure in the leaf nodes.</li> </ul>"}]}