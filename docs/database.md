# Database

## PostgreSQL

### Specialties

* Multi-version concurrency control
    * i.e. A row of records is versioned into multiple tuples that are representing the same logical row but in different states/versions. The newest version represents the up-to-date record for that row. Different processes can access a record concurrently.
* Can define custom type
    * ProgreSQL allows users to define custome types/objects in a table schema.

### How It Works Internally

_Study notes from [presentation by Hussein Nasser](https://youtu.be/Q56kljmIN14?si=zSISqnNQNV-7KUJe)._

#### Overview

![postgres-architecture](./images/postgres-architecture.png)

#### Post Master Process

A parent process that starts at the early stage of the application. Exposes the application to port 5432, such that it's ready for connection.

#### Backend Processes

Each backend process is responsible for maintaining a connection to its consumer.

The number of backend process is capped by the parameter `max_connections`.

#### Background Workers

A background worker is responsible for executing the query or command that a consumer initiated.

The number of background workers is capped by the parameter `max_worker_processes`.

#### Background Writers

A background writer is responsible for flushing the data that is stored in a Page into the filesystem, which will eventually write the data into the disk. It wakes up occasionally to clean up dirty Pages/Shared Memory.

#### Checkpointer

Checkpointer is responsible for flushing everything - i.e. both WAL records and Pages to the disk, and creating a checkpoint, indicating that everything now is consistent.

#### Logger

Logger is responsible for writing all the logs into the disk.

#### Autovacuum Launcher and Workers

Autovacuum worker is responsible for cleaning up the tuples that have older versions and are not being accessed by any of the processes.

The number of autovacuum works is capped by the parameter `autovacuum_max_workers`.

#### WAL Archiver

WAL Archiver is responsible for archiving all the WAL records for historical purposes, such that a database can be brought to the newest state from 0 when needed.

#### WAL Reciever

WAL Reciever is responsible for recieving WAL records.

#### WAL Writer

WAL Writer is responsible for flushing the WAL records into the disk. Each time a commit succeeds, WAL records are flushed into the disk.

#### WAL Sender

A WAL Sender is responsible for sending the WAL records to the replicas, from the master instance.

The number of WAL Senders is capped by the parameter `max_val_senders`.

#### Startup Process

The Startup Process is responsible for checking the state of the database - i.e. it checks whether the data stored in the Pages are up-to-date with what has been recorded in the WAL records. If the Pages are found out of date, the Startup process will try to re-do what has been recorded in the WAL records, in order to load the up-to-date data into the Pages, before the Post Master Process is started.

### Performance Tips

The following tips for performance improvement is not tight to a specific DB solution. They can be applied to many of the DB solutions nowadays.

#### Prepared Statements

Like GraphQL, each document attached in a request needs to be parsed and validated before execution. If a statement is prepared, the same requests that come with the same statement can skip this process, and thus improves the performance.

However, prepared statements only work in the same session. If a statement is not prepared in a session, it needs to be prepared before execution.

This also doesn't help in performance if a statement itself is time consuming - e.g. loading a large amount of data.

#### Index

This is also a common technique in database to accelerate data lookup. This technique works similarly to a hash table where the key is generated by the specified column, and the value is the corresponding row of records.

#### Partition

This technique segments the data into different partitions according to the column(s) specified, such that queries can scan only the partitions of data that are interested without scanning through all the records one by one.

However, be sure to benchmark the performance before determining which columns to use for partitioning. Scanning through a large amount of partitions would not result in performance improvement.

Also, be sure to keep the partitions up-to-date when data are updated in the database. A common practice is to leverage cron job to regularly update the partitions created such that queries run against the partitions would not be performed with stale data.

#### Copy

To improve INSERT performance for a large amount of data, the COPY command can be used to import the data from either a file (e.g. a csv file) or standard input to the database.

#### Separation of Concerns

This is basically an idea of horizontal scaling, where READ replicas are created to take care of the READ concerns specifically, such that the main instance of the DB can focus on scaling for the WRITE operations. This is usually a technique used when the amount of READ operations is significantly larger than the WRITE operations.

